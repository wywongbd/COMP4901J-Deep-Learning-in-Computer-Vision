{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based RL\n",
    "In this exercise you will implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we going to accomplish this in Tensorflow? We are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agentâ€™s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "\n",
    "################################################################################\n",
    "# TODO: Implement the loss function.                                           #\n",
    "# This sends the weights in the direction of making actions that gave good     #\n",
    "# advantage (reward overtime) more likely, and actions that didn't less likely.#\n",
    "################################################################################\n",
    "log_likelihood = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(log_likelihood * advantages) \n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "def discount_rewards(r):\n",
    "    ################################################################################\n",
    "    # TODO: Implement the discounted rewards function                              #\n",
    "    # Return discounted rewards weighed by gamma. Each reward will be replaced     #\n",
    "    # with a weight reward that involves itself and all the other rewards occuring #\n",
    "    # after it. The later the reward after it happens, the less effect it has on   #\n",
    "    # the current rewards's discounted reward                                      #\n",
    "    # Hint: [r0, r1, r2, ..., r_N] will look someting like:                        #\n",
    "    #       [(r0 + r1*gamma^1 + ... r_N*gamma^N), (r1 + r2*gamma^1 + ...), ...]    #\n",
    "    ################################################################################\n",
    "    discounted_rewards = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 16.333333. action: 0.000000. mean reward 16.333333.\n",
      "World Perf: Episode 7.000000. Reward 17.666667. action: 0.000000. mean reward 16.346667.\n",
      "World Perf: Episode 10.000000. Reward 17.333333. action: 1.000000. mean reward 16.356533.\n",
      "World Perf: Episode 13.000000. Reward 15.000000. action: 1.000000. mean reward 16.342968.\n",
      "World Perf: Episode 16.000000. Reward 18.666667. action: 0.000000. mean reward 16.366205.\n",
      "World Perf: Episode 19.000000. Reward 13.666667. action: 0.000000. mean reward 16.339210.\n",
      "World Perf: Episode 22.000000. Reward 19.333333. action: 0.000000. mean reward 16.369151.\n",
      "World Perf: Episode 25.000000. Reward 19.666667. action: 0.000000. mean reward 16.402126.\n",
      "World Perf: Episode 28.000000. Reward 19.333333. action: 1.000000. mean reward 16.431438.\n",
      "World Perf: Episode 31.000000. Reward 14.000000. action: 1.000000. mean reward 16.407124.\n",
      "World Perf: Episode 34.000000. Reward 22.333333. action: 1.000000. mean reward 16.466386.\n",
      "World Perf: Episode 37.000000. Reward 27.333333. action: 0.000000. mean reward 16.575055.\n",
      "World Perf: Episode 40.000000. Reward 29.000000. action: 0.000000. mean reward 16.699305.\n",
      "World Perf: Episode 43.000000. Reward 21.000000. action: 1.000000. mean reward 16.742312.\n",
      "World Perf: Episode 46.000000. Reward 16.000000. action: 1.000000. mean reward 16.734889.\n",
      "World Perf: Episode 49.000000. Reward 16.000000. action: 1.000000. mean reward 16.727540.\n",
      "World Perf: Episode 52.000000. Reward 23.666667. action: 0.000000. mean reward 16.796931.\n",
      "World Perf: Episode 55.000000. Reward 22.000000. action: 1.000000. mean reward 16.848962.\n",
      "World Perf: Episode 58.000000. Reward 16.000000. action: 1.000000. mean reward 16.840472.\n",
      "World Perf: Episode 61.000000. Reward 19.666667. action: 1.000000. mean reward 16.868734.\n",
      "World Perf: Episode 64.000000. Reward 15.666667. action: 0.000000. mean reward 16.856713.\n",
      "World Perf: Episode 67.000000. Reward 15.000000. action: 1.000000. mean reward 16.838146.\n",
      "World Perf: Episode 70.000000. Reward 16.000000. action: 0.000000. mean reward 16.829765.\n",
      "World Perf: Episode 73.000000. Reward 13.333333. action: 0.000000. mean reward 16.794800.\n",
      "World Perf: Episode 76.000000. Reward 18.666667. action: 0.000000. mean reward 16.813519.\n",
      "World Perf: Episode 79.000000. Reward 15.666667. action: 1.000000. mean reward 16.802051.\n",
      "World Perf: Episode 82.000000. Reward 22.000000. action: 0.000000. mean reward 16.854030.\n",
      "World Perf: Episode 85.000000. Reward 13.000000. action: 0.000000. mean reward 16.815490.\n",
      "World Perf: Episode 88.000000. Reward 16.666667. action: 0.000000. mean reward 16.814001.\n",
      "World Perf: Episode 91.000000. Reward 13.666667. action: 0.000000. mean reward 16.782528.\n",
      "World Perf: Episode 94.000000. Reward 17.333333. action: 0.000000. mean reward 16.788036.\n",
      "World Perf: Episode 97.000000. Reward 14.333333. action: 0.000000. mean reward 16.763489.\n",
      "World Perf: Episode 100.000000. Reward 20.333333. action: 1.000000. mean reward 16.799188.\n",
      "World Perf: Episode 103.000000. Reward 22.333333. action: 1.000000. mean reward 16.854529.\n",
      "World Perf: Episode 106.000000. Reward 14.333333. action: 0.000000. mean reward 16.602104.\n",
      "World Perf: Episode 109.000000. Reward 17.333333. action: 1.000000. mean reward 16.412975.\n",
      "World Perf: Episode 112.000000. Reward 26.333333. action: 0.000000. mean reward 22.734947.\n",
      "World Perf: Episode 115.000000. Reward 16.666667. action: 1.000000. mean reward 28.005178.\n",
      "World Perf: Episode 118.000000. Reward 21.000000. action: 0.000000. mean reward 32.548634.\n",
      "World Perf: Episode 121.000000. Reward 12.666667. action: 0.000000. mean reward 31.990927.\n",
      "World Perf: Episode 124.000000. Reward 36.000000. action: 1.000000. mean reward 31.488661.\n",
      "World Perf: Episode 127.000000. Reward 18.000000. action: 1.000000. mean reward 30.810631.\n",
      "World Perf: Episode 130.000000. Reward 22.000000. action: 0.000000. mean reward 30.261314.\n",
      "World Perf: Episode 133.000000. Reward 16.333333. action: 0.000000. mean reward 35.706322.\n",
      "World Perf: Episode 136.000000. Reward 15.000000. action: 1.000000. mean reward 41.469822.\n",
      "World Perf: Episode 139.000000. Reward 32.000000. action: 0.000000. mean reward 41.002419.\n",
      "World Perf: Episode 142.000000. Reward 19.000000. action: 1.000000. mean reward 41.283573.\n",
      "World Perf: Episode 145.000000. Reward 13.666667. action: 0.000000. mean reward 45.251308.\n",
      "World Perf: Episode 148.000000. Reward 25.000000. action: 1.000000. mean reward 49.815441.\n",
      "World Perf: Episode 151.000000. Reward 23.333333. action: 1.000000. mean reward 48.756199.\n",
      "World Perf: Episode 154.000000. Reward 20.000000. action: 1.000000. mean reward 47.595379.\n",
      "World Perf: Episode 157.000000. Reward 31.000000. action: 1.000000. mean reward 46.597836.\n",
      "World Perf: Episode 160.000000. Reward 16.666667. action: 0.000000. mean reward 45.493839.\n",
      "World Perf: Episode 163.000000. Reward 15.333333. action: 1.000000. mean reward 44.643757.\n",
      "World Perf: Episode 166.000000. Reward 31.333333. action: 1.000000. mean reward 43.794903.\n",
      "World Perf: Episode 169.000000. Reward 24.666667. action: 1.000000. mean reward 43.082886.\n",
      "World Perf: Episode 172.000000. Reward 26.333333. action: 0.000000. mean reward 42.610352.\n",
      "World Perf: Episode 175.000000. Reward 29.000000. action: 0.000000. mean reward 41.981373.\n",
      "World Perf: Episode 178.000000. Reward 27.000000. action: 0.000000. mean reward 41.485016.\n",
      "World Perf: Episode 181.000000. Reward 33.333333. action: 1.000000. mean reward 40.753918.\n",
      "World Perf: Episode 184.000000. Reward 20.000000. action: 1.000000. mean reward 40.046215.\n",
      "World Perf: Episode 187.000000. Reward 18.000000. action: 0.000000. mean reward 39.351223.\n",
      "World Perf: Episode 190.000000. Reward 21.000000. action: 1.000000. mean reward 39.308575.\n",
      "World Perf: Episode 193.000000. Reward 29.333333. action: 1.000000. mean reward 38.580647.\n",
      "World Perf: Episode 196.000000. Reward 40.000000. action: 1.000000. mean reward 38.559166.\n",
      "World Perf: Episode 199.000000. Reward 37.333333. action: 0.000000. mean reward 39.655628.\n",
      "World Perf: Episode 202.000000. Reward 24.000000. action: 0.000000. mean reward 39.446964.\n",
      "World Perf: Episode 205.000000. Reward 24.333333. action: 0.000000. mean reward 39.891010.\n",
      "World Perf: Episode 208.000000. Reward 41.333333. action: 0.000000. mean reward 39.359138.\n",
      "World Perf: Episode 211.000000. Reward 19.666667. action: 1.000000. mean reward 38.655487.\n",
      "World Perf: Episode 214.000000. Reward 32.000000. action: 0.000000. mean reward 38.167786.\n",
      "World Perf: Episode 217.000000. Reward 20.333333. action: 1.000000. mean reward 37.821941.\n",
      "World Perf: Episode 220.000000. Reward 41.666667. action: 1.000000. mean reward 39.774441.\n",
      "World Perf: Episode 223.000000. Reward 20.000000. action: 0.000000. mean reward 40.475056.\n",
      "World Perf: Episode 226.000000. Reward 32.666667. action: 0.000000. mean reward 39.924637.\n",
      "World Perf: Episode 229.000000. Reward 16.666667. action: 1.000000. mean reward 39.238243.\n",
      "World Perf: Episode 232.000000. Reward 43.333333. action: 0.000000. mean reward 45.043583.\n",
      "World Perf: Episode 235.000000. Reward 47.000000. action: 0.000000. mean reward 44.383228.\n",
      "World Perf: Episode 238.000000. Reward 34.333333. action: 0.000000. mean reward 43.704052.\n",
      "World Perf: Episode 241.000000. Reward 21.666667. action: 1.000000. mean reward 47.075184.\n",
      "World Perf: Episode 244.000000. Reward 42.000000. action: 0.000000. mean reward 46.560581.\n",
      "World Perf: Episode 247.000000. Reward 26.666667. action: 0.000000. mean reward 45.821823.\n",
      "World Perf: Episode 250.000000. Reward 31.666667. action: 0.000000. mean reward 45.247974.\n",
      "World Perf: Episode 253.000000. Reward 29.000000. action: 0.000000. mean reward 45.214100.\n",
      "World Perf: Episode 256.000000. Reward 35.000000. action: 1.000000. mean reward 44.418163.\n",
      "World Perf: Episode 259.000000. Reward 25.666667. action: 0.000000. mean reward 44.367832.\n",
      "World Perf: Episode 262.000000. Reward 28.666667. action: 0.000000. mean reward 44.906292.\n",
      "World Perf: Episode 265.000000. Reward 67.666667. action: 0.000000. mean reward 44.616680.\n",
      "World Perf: Episode 268.000000. Reward 27.666667. action: 1.000000. mean reward 43.745106.\n",
      "World Perf: Episode 271.000000. Reward 27.666667. action: 0.000000. mean reward 43.992985.\n",
      "World Perf: Episode 274.000000. Reward 32.333333. action: 0.000000. mean reward 43.624207.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 277.000000. Reward 23.000000. action: 0.000000. mean reward 43.294804.\n",
      "World Perf: Episode 280.000000. Reward 15.333333. action: 0.000000. mean reward 43.106709.\n",
      "World Perf: Episode 283.000000. Reward 37.333333. action: 1.000000. mean reward 42.615162.\n",
      "World Perf: Episode 286.000000. Reward 30.000000. action: 1.000000. mean reward 42.123035.\n",
      "World Perf: Episode 289.000000. Reward 39.666667. action: 1.000000. mean reward 41.930809.\n",
      "World Perf: Episode 292.000000. Reward 29.000000. action: 1.000000. mean reward 41.550076.\n",
      "World Perf: Episode 295.000000. Reward 31.333333. action: 1.000000. mean reward 40.970005.\n",
      "World Perf: Episode 298.000000. Reward 33.666667. action: 0.000000. mean reward 40.422783.\n",
      "World Perf: Episode 301.000000. Reward 32.000000. action: 1.000000. mean reward 39.886715.\n",
      "World Perf: Episode 304.000000. Reward 35.333333. action: 1.000000. mean reward 39.216000.\n",
      "World Perf: Episode 307.000000. Reward 25.333333. action: 0.000000. mean reward 41.908127.\n",
      "World Perf: Episode 310.000000. Reward 22.000000. action: 1.000000. mean reward 41.175739.\n",
      "World Perf: Episode 313.000000. Reward 36.000000. action: 1.000000. mean reward 40.586193.\n",
      "World Perf: Episode 316.000000. Reward 26.333333. action: 1.000000. mean reward 40.816822.\n",
      "World Perf: Episode 319.000000. Reward 24.000000. action: 1.000000. mean reward 40.802177.\n",
      "World Perf: Episode 322.000000. Reward 29.666667. action: 0.000000. mean reward 40.505833.\n",
      "World Perf: Episode 325.000000. Reward 28.333333. action: 1.000000. mean reward 41.629097.\n",
      "World Perf: Episode 328.000000. Reward 34.666667. action: 0.000000. mean reward 43.241089.\n",
      "World Perf: Episode 331.000000. Reward 18.000000. action: 1.000000. mean reward 43.559418.\n",
      "World Perf: Episode 334.000000. Reward 32.000000. action: 0.000000. mean reward 47.041836.\n",
      "World Perf: Episode 337.000000. Reward 40.666667. action: 1.000000. mean reward 46.485149.\n",
      "World Perf: Episode 340.000000. Reward 32.666667. action: 0.000000. mean reward 46.293533.\n",
      "World Perf: Episode 343.000000. Reward 38.333333. action: 1.000000. mean reward 45.963821.\n",
      "World Perf: Episode 346.000000. Reward 47.666667. action: 1.000000. mean reward 46.718914.\n",
      "World Perf: Episode 349.000000. Reward 35.000000. action: 1.000000. mean reward 51.540085.\n",
      "World Perf: Episode 352.000000. Reward 39.000000. action: 1.000000. mean reward 51.016422.\n",
      "World Perf: Episode 355.000000. Reward 30.666667. action: 0.000000. mean reward 50.259064.\n",
      "World Perf: Episode 358.000000. Reward 52.000000. action: 1.000000. mean reward 49.604660.\n",
      "World Perf: Episode 361.000000. Reward 27.000000. action: 0.000000. mean reward 49.334896.\n",
      "World Perf: Episode 364.000000. Reward 44.333333. action: 1.000000. mean reward 48.597610.\n",
      "World Perf: Episode 367.000000. Reward 38.666667. action: 1.000000. mean reward 48.094151.\n",
      "World Perf: Episode 370.000000. Reward 30.666667. action: 0.000000. mean reward 47.358364.\n",
      "World Perf: Episode 373.000000. Reward 42.000000. action: 1.000000. mean reward 47.508900.\n",
      "World Perf: Episode 376.000000. Reward 57.333333. action: 0.000000. mean reward 46.834652.\n",
      "World Perf: Episode 379.000000. Reward 51.000000. action: 0.000000. mean reward 46.109470.\n",
      "World Perf: Episode 382.000000. Reward 41.333333. action: 0.000000. mean reward 45.403030.\n",
      "World Perf: Episode 385.000000. Reward 45.000000. action: 0.000000. mean reward 46.457932.\n",
      "World Perf: Episode 388.000000. Reward 47.333333. action: 1.000000. mean reward 45.906513.\n",
      "World Perf: Episode 391.000000. Reward 42.666667. action: 1.000000. mean reward 45.384327.\n",
      "World Perf: Episode 394.000000. Reward 34.333333. action: 1.000000. mean reward 44.608017.\n",
      "World Perf: Episode 397.000000. Reward 39.333333. action: 0.000000. mean reward 44.133572.\n",
      "World Perf: Episode 400.000000. Reward 84.000000. action: 0.000000. mean reward 49.692310.\n",
      "World Perf: Episode 403.000000. Reward 45.666667. action: 0.000000. mean reward 51.809101.\n",
      "World Perf: Episode 406.000000. Reward 34.666667. action: 1.000000. mean reward 51.045132.\n",
      "World Perf: Episode 409.000000. Reward 50.333333. action: 1.000000. mean reward 51.271366.\n",
      "World Perf: Episode 412.000000. Reward 35.000000. action: 1.000000. mean reward 51.027454.\n",
      "World Perf: Episode 415.000000. Reward 45.333333. action: 1.000000. mean reward 50.709568.\n",
      "World Perf: Episode 418.000000. Reward 30.666667. action: 1.000000. mean reward 50.167088.\n",
      "World Perf: Episode 421.000000. Reward 36.666667. action: 1.000000. mean reward 49.657608.\n",
      "World Perf: Episode 424.000000. Reward 45.333333. action: 1.000000. mean reward 52.873108.\n",
      "World Perf: Episode 427.000000. Reward 56.000000. action: 1.000000. mean reward 52.901081.\n",
      "World Perf: Episode 430.000000. Reward 42.666667. action: 1.000000. mean reward 52.029209.\n",
      "World Perf: Episode 433.000000. Reward 40.000000. action: 1.000000. mean reward 51.472637.\n",
      "World Perf: Episode 436.000000. Reward 53.333333. action: 0.000000. mean reward 52.936977.\n",
      "World Perf: Episode 439.000000. Reward 62.333333. action: 1.000000. mean reward 52.506535.\n",
      "World Perf: Episode 442.000000. Reward 32.666667. action: 1.000000. mean reward 52.001553.\n",
      "World Perf: Episode 445.000000. Reward 46.666667. action: 0.000000. mean reward 51.319504.\n",
      "World Perf: Episode 448.000000. Reward 55.666667. action: 1.000000. mean reward 50.737469.\n",
      "World Perf: Episode 451.000000. Reward 48.333333. action: 0.000000. mean reward 52.558960.\n",
      "World Perf: Episode 454.000000. Reward 56.333333. action: 0.000000. mean reward 51.890514.\n",
      "World Perf: Episode 457.000000. Reward 62.333333. action: 0.000000. mean reward 52.446980.\n",
      "World Perf: Episode 460.000000. Reward 41.000000. action: 1.000000. mean reward 51.873058.\n",
      "World Perf: Episode 463.000000. Reward 50.000000. action: 0.000000. mean reward 56.785412.\n",
      "World Perf: Episode 466.000000. Reward 32.666667. action: 0.000000. mean reward 56.712879.\n",
      "World Perf: Episode 469.000000. Reward 62.333333. action: 1.000000. mean reward 55.905888.\n",
      "World Perf: Episode 472.000000. Reward 37.333333. action: 0.000000. mean reward 55.354294.\n",
      "World Perf: Episode 475.000000. Reward 27.333333. action: 0.000000. mean reward 56.614899.\n",
      "World Perf: Episode 478.000000. Reward 43.000000. action: 0.000000. mean reward 55.912067.\n",
      "World Perf: Episode 481.000000. Reward 84.000000. action: 0.000000. mean reward 56.521988.\n",
      "World Perf: Episode 484.000000. Reward 51.000000. action: 1.000000. mean reward 55.938770.\n",
      "World Perf: Episode 487.000000. Reward 45.333333. action: 0.000000. mean reward 55.095032.\n",
      "World Perf: Episode 490.000000. Reward 54.000000. action: 0.000000. mean reward 54.266754.\n",
      "World Perf: Episode 493.000000. Reward 62.666667. action: 0.000000. mean reward 55.333851.\n",
      "World Perf: Episode 496.000000. Reward 61.666667. action: 0.000000. mean reward 55.514362.\n",
      "World Perf: Episode 499.000000. Reward 89.666667. action: 1.000000. mean reward 55.115971.\n",
      "World Perf: Episode 502.000000. Reward 44.000000. action: 1.000000. mean reward 58.969563.\n",
      "World Perf: Episode 505.000000. Reward 39.666667. action: 0.000000. mean reward 58.759312.\n",
      "World Perf: Episode 508.000000. Reward 66.000000. action: 0.000000. mean reward 57.937794.\n",
      "World Perf: Episode 511.000000. Reward 51.666667. action: 0.000000. mean reward 56.918354.\n",
      "World Perf: Episode 514.000000. Reward 49.000000. action: 0.000000. mean reward 55.849110.\n",
      "World Perf: Episode 517.000000. Reward 19.000000. action: 0.000000. mean reward 54.598515.\n",
      "World Perf: Episode 520.000000. Reward 66.000000. action: 1.000000. mean reward 53.876431.\n",
      "World Perf: Episode 523.000000. Reward 45.000000. action: 0.000000. mean reward 53.108189.\n",
      "World Perf: Episode 526.000000. Reward 63.000000. action: 1.000000. mean reward 52.582947.\n",
      "World Perf: Episode 529.000000. Reward 60.666667. action: 1.000000. mean reward 52.216175.\n",
      "World Perf: Episode 532.000000. Reward 45.333333. action: 1.000000. mean reward 51.927250.\n",
      "World Perf: Episode 535.000000. Reward 54.666667. action: 0.000000. mean reward 51.147949.\n",
      "World Perf: Episode 538.000000. Reward 36.666667. action: 0.000000. mean reward 52.581280.\n",
      "World Perf: Episode 541.000000. Reward 49.000000. action: 1.000000. mean reward 51.702946.\n",
      "World Perf: Episode 544.000000. Reward 44.000000. action: 0.000000. mean reward 50.747971.\n",
      "World Perf: Episode 547.000000. Reward 31.000000. action: 1.000000. mean reward 49.736065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 550.000000. Reward 64.666667. action: 1.000000. mean reward 50.015697.\n",
      "World Perf: Episode 553.000000. Reward 50.000000. action: 1.000000. mean reward 49.813389.\n",
      "World Perf: Episode 556.000000. Reward 28.000000. action: 1.000000. mean reward 48.717789.\n",
      "World Perf: Episode 559.000000. Reward 44.000000. action: 1.000000. mean reward 47.863148.\n",
      "World Perf: Episode 562.000000. Reward 65.666667. action: 1.000000. mean reward 47.399490.\n",
      "World Perf: Episode 565.000000. Reward 40.000000. action: 0.000000. mean reward 46.753376.\n",
      "World Perf: Episode 568.000000. Reward 53.333333. action: 1.000000. mean reward 46.020245.\n",
      "World Perf: Episode 571.000000. Reward 38.333333. action: 1.000000. mean reward 45.335697.\n",
      "World Perf: Episode 574.000000. Reward 38.333333. action: 1.000000. mean reward 44.657215.\n",
      "World Perf: Episode 577.000000. Reward 51.000000. action: 1.000000. mean reward 49.343185.\n",
      "World Perf: Episode 580.000000. Reward 61.333333. action: 1.000000. mean reward 48.825939.\n",
      "World Perf: Episode 583.000000. Reward 66.333333. action: 1.000000. mean reward 48.321758.\n",
      "World Perf: Episode 586.000000. Reward 72.000000. action: 0.000000. mean reward 47.835232.\n",
      "World Perf: Episode 589.000000. Reward 44.666667. action: 0.000000. mean reward 47.069088.\n",
      "World Perf: Episode 592.000000. Reward 61.333333. action: 1.000000. mean reward 46.429173.\n",
      "World Perf: Episode 595.000000. Reward 62.666667. action: 1.000000. mean reward 46.115078.\n",
      "World Perf: Episode 598.000000. Reward 49.000000. action: 0.000000. mean reward 45.828518.\n",
      "World Perf: Episode 601.000000. Reward 39.333333. action: 0.000000. mean reward 50.781937.\n",
      "World Perf: Episode 604.000000. Reward 75.000000. action: 0.000000. mean reward 50.779587.\n",
      "World Perf: Episode 607.000000. Reward 79.000000. action: 1.000000. mean reward 50.310089.\n",
      "World Perf: Episode 610.000000. Reward 58.333333. action: 0.000000. mean reward 50.706440.\n",
      "World Perf: Episode 613.000000. Reward 65.666667. action: 1.000000. mean reward 50.737213.\n",
      "World Perf: Episode 616.000000. Reward 109.333333. action: 0.000000. mean reward 51.325741.\n",
      "World Perf: Episode 619.000000. Reward 49.000000. action: 0.000000. mean reward 51.245956.\n",
      "World Perf: Episode 622.000000. Reward 62.000000. action: 0.000000. mean reward 50.582699.\n",
      "World Perf: Episode 625.000000. Reward 96.000000. action: 0.000000. mean reward 50.231583.\n",
      "World Perf: Episode 628.000000. Reward 76.000000. action: 0.000000. mean reward 49.632065.\n",
      "World Perf: Episode 631.000000. Reward 80.000000. action: 1.000000. mean reward 49.588928.\n",
      "World Perf: Episode 634.000000. Reward 65.666667. action: 0.000000. mean reward 48.990231.\n",
      "World Perf: Episode 637.000000. Reward 42.666667. action: 1.000000. mean reward 48.174953.\n",
      "World Perf: Episode 640.000000. Reward 42.666667. action: 0.000000. mean reward 47.860050.\n",
      "World Perf: Episode 643.000000. Reward 60.333333. action: 1.000000. mean reward 47.597019.\n",
      "World Perf: Episode 646.000000. Reward 103.000000. action: 0.000000. mean reward 47.369171.\n",
      "World Perf: Episode 649.000000. Reward 60.666667. action: 1.000000. mean reward 48.668839.\n",
      "World Perf: Episode 652.000000. Reward 50.333333. action: 0.000000. mean reward 48.055668.\n",
      "World Perf: Episode 655.000000. Reward 107.333333. action: 1.000000. mean reward 56.224838.\n",
      "World Perf: Episode 658.000000. Reward 97.333333. action: 1.000000. mean reward 56.833374.\n",
      "World Perf: Episode 661.000000. Reward 55.333333. action: 0.000000. mean reward 57.573685.\n",
      "World Perf: Episode 664.000000. Reward 67.000000. action: 0.000000. mean reward 58.181118.\n",
      "World Perf: Episode 667.000000. Reward 56.000000. action: 0.000000. mean reward 57.527866.\n",
      "World Perf: Episode 670.000000. Reward 102.333333. action: 0.000000. mean reward 62.852585.\n",
      "World Perf: Episode 673.000000. Reward 78.333333. action: 1.000000. mean reward 62.279438.\n",
      "World Perf: Episode 676.000000. Reward 79.666667. action: 1.000000. mean reward 67.242729.\n",
      "World Perf: Episode 679.000000. Reward 84.000000. action: 1.000000. mean reward 71.207878.\n",
      "World Perf: Episode 682.000000. Reward 46.000000. action: 1.000000. mean reward 69.872078.\n",
      "World Perf: Episode 685.000000. Reward 61.333333. action: 0.000000. mean reward 68.974991.\n",
      "World Perf: Episode 688.000000. Reward 62.000000. action: 1.000000. mean reward 67.803711.\n",
      "World Perf: Episode 691.000000. Reward 57.333333. action: 1.000000. mean reward 70.042412.\n",
      "World Perf: Episode 694.000000. Reward 38.000000. action: 1.000000. mean reward 69.207520.\n",
      "World Perf: Episode 697.000000. Reward 70.666667. action: 0.000000. mean reward 72.522636.\n",
      "World Perf: Episode 700.000000. Reward 110.000000. action: 1.000000. mean reward 71.865952.\n",
      "World Perf: Episode 703.000000. Reward 49.333333. action: 0.000000. mean reward 70.527412.\n",
      "World Perf: Episode 706.000000. Reward 63.000000. action: 1.000000. mean reward 69.345322.\n",
      "World Perf: Episode 709.000000. Reward 69.666667. action: 1.000000. mean reward 69.332802.\n",
      "World Perf: Episode 712.000000. Reward 58.666667. action: 1.000000. mean reward 68.331299.\n",
      "World Perf: Episode 715.000000. Reward 45.666667. action: 0.000000. mean reward 67.635834.\n",
      "World Perf: Episode 718.000000. Reward 72.000000. action: 1.000000. mean reward 66.715935.\n",
      "World Perf: Episode 721.000000. Reward 59.333333. action: 1.000000. mean reward 65.881432.\n",
      "World Perf: Episode 724.000000. Reward 63.333333. action: 0.000000. mean reward 66.054840.\n",
      "World Perf: Episode 727.000000. Reward 120.000000. action: 0.000000. mean reward 66.533630.\n",
      "World Perf: Episode 730.000000. Reward 48.000000. action: 1.000000. mean reward 69.408958.\n",
      "World Perf: Episode 733.000000. Reward 62.333333. action: 1.000000. mean reward 68.285713.\n",
      "World Perf: Episode 736.000000. Reward 53.666667. action: 0.000000. mean reward 72.592140.\n",
      "World Perf: Episode 739.000000. Reward 80.000000. action: 1.000000. mean reward 71.674461.\n",
      "World Perf: Episode 742.000000. Reward 42.333333. action: 1.000000. mean reward 71.782898.\n",
      "World Perf: Episode 745.000000. Reward 86.333333. action: 1.000000. mean reward 70.959328.\n",
      "World Perf: Episode 748.000000. Reward 49.000000. action: 0.000000. mean reward 70.546913.\n",
      "World Perf: Episode 751.000000. Reward 80.666667. action: 1.000000. mean reward 69.567101.\n",
      "World Perf: Episode 754.000000. Reward 81.333333. action: 1.000000. mean reward 68.654610.\n",
      "World Perf: Episode 757.000000. Reward 64.333333. action: 0.000000. mean reward 67.400322.\n",
      "World Perf: Episode 760.000000. Reward 50.333333. action: 1.000000. mean reward 67.942841.\n",
      "World Perf: Episode 763.000000. Reward 96.666667. action: 1.000000. mean reward 68.043129.\n",
      "World Perf: Episode 766.000000. Reward 78.666667. action: 1.000000. mean reward 67.541664.\n",
      "World Perf: Episode 769.000000. Reward 71.000000. action: 1.000000. mean reward 67.328720.\n",
      "World Perf: Episode 772.000000. Reward 78.000000. action: 1.000000. mean reward 66.464638.\n",
      "World Perf: Episode 775.000000. Reward 63.666667. action: 1.000000. mean reward 65.726334.\n",
      "World Perf: Episode 778.000000. Reward 80.000000. action: 1.000000. mean reward 65.926003.\n",
      "World Perf: Episode 781.000000. Reward 73.333333. action: 0.000000. mean reward 70.650391.\n",
      "World Perf: Episode 784.000000. Reward 46.666667. action: 1.000000. mean reward 69.750771.\n",
      "World Perf: Episode 787.000000. Reward 63.000000. action: 1.000000. mean reward 68.838081.\n",
      "World Perf: Episode 790.000000. Reward 48.333333. action: 1.000000. mean reward 67.683891.\n",
      "World Perf: Episode 793.000000. Reward 90.333333. action: 1.000000. mean reward 71.520370.\n",
      "World Perf: Episode 796.000000. Reward 89.666667. action: 1.000000. mean reward 71.902489.\n",
      "World Perf: Episode 799.000000. Reward 66.666667. action: 1.000000. mean reward 71.010376.\n",
      "World Perf: Episode 802.000000. Reward 66.000000. action: 1.000000. mean reward 69.902924.\n",
      "World Perf: Episode 805.000000. Reward 62.333333. action: 0.000000. mean reward 68.874046.\n",
      "World Perf: Episode 808.000000. Reward 105.333333. action: 1.000000. mean reward 68.251938.\n",
      "World Perf: Episode 811.000000. Reward 82.666667. action: 0.000000. mean reward 67.947243.\n",
      "World Perf: Episode 814.000000. Reward 72.666667. action: 1.000000. mean reward 67.667290.\n",
      "World Perf: Episode 817.000000. Reward 135.666667. action: 0.000000. mean reward 68.169662.\n",
      "World Perf: Episode 820.000000. Reward 84.000000. action: 0.000000. mean reward 67.661697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 823.000000. Reward 61.666667. action: 0.000000. mean reward 67.606377.\n",
      "World Perf: Episode 826.000000. Reward 96.666667. action: 1.000000. mean reward 67.331383.\n",
      "World Perf: Episode 829.000000. Reward 82.666667. action: 0.000000. mean reward 66.769325.\n",
      "World Perf: Episode 832.000000. Reward 91.666667. action: 1.000000. mean reward 66.726555.\n",
      "World Perf: Episode 835.000000. Reward 87.333333. action: 1.000000. mean reward 66.222450.\n",
      "World Perf: Episode 838.000000. Reward 94.666667. action: 0.000000. mean reward 66.327827.\n",
      "World Perf: Episode 841.000000. Reward 109.666667. action: 0.000000. mean reward 66.798378.\n",
      "World Perf: Episode 844.000000. Reward 119.333333. action: 1.000000. mean reward 70.010338.\n",
      "World Perf: Episode 847.000000. Reward 92.333333. action: 1.000000. mean reward 74.791069.\n",
      "World Perf: Episode 850.000000. Reward 90.666667. action: 1.000000. mean reward 74.063164.\n",
      "World Perf: Episode 853.000000. Reward 51.333333. action: 0.000000. mean reward 72.754761.\n",
      "World Perf: Episode 856.000000. Reward 88.666667. action: 1.000000. mean reward 72.931389.\n",
      "World Perf: Episode 859.000000. Reward 98.000000. action: 0.000000. mean reward 76.977478.\n",
      "World Perf: Episode 862.000000. Reward 79.666667. action: 0.000000. mean reward 76.501869.\n",
      "World Perf: Episode 865.000000. Reward 83.666667. action: 0.000000. mean reward 76.522789.\n",
      "World Perf: Episode 868.000000. Reward 112.000000. action: 0.000000. mean reward 76.049294.\n",
      "World Perf: Episode 871.000000. Reward 113.000000. action: 1.000000. mean reward 78.583641.\n",
      "World Perf: Episode 874.000000. Reward 91.666667. action: 1.000000. mean reward 77.629410.\n",
      "World Perf: Episode 877.000000. Reward 72.000000. action: 0.000000. mean reward 76.446114.\n",
      "World Perf: Episode 880.000000. Reward 109.333333. action: 1.000000. mean reward 79.710457.\n",
      "World Perf: Episode 883.000000. Reward 110.000000. action: 0.000000. mean reward 79.062561.\n",
      "World Perf: Episode 886.000000. Reward 91.666667. action: 0.000000. mean reward 78.700081.\n",
      "World Perf: Episode 889.000000. Reward 119.666667. action: 0.000000. mean reward 82.044289.\n",
      "World Perf: Episode 892.000000. Reward 81.333333. action: 0.000000. mean reward 83.294075.\n",
      "World Perf: Episode 895.000000. Reward 108.333333. action: 1.000000. mean reward 82.355515.\n",
      "World Perf: Episode 898.000000. Reward 57.000000. action: 0.000000. mean reward 80.740135.\n",
      "World Perf: Episode 901.000000. Reward 91.666667. action: 1.000000. mean reward 79.812080.\n",
      "World Perf: Episode 904.000000. Reward 66.333333. action: 0.000000. mean reward 78.489586.\n",
      "World Perf: Episode 907.000000. Reward 92.333333. action: 1.000000. mean reward 78.272507.\n",
      "World Perf: Episode 910.000000. Reward 89.333333. action: 0.000000. mean reward 77.839066.\n",
      "World Perf: Episode 913.000000. Reward 86.000000. action: 0.000000. mean reward 77.212845.\n",
      "World Perf: Episode 916.000000. Reward 118.333333. action: 0.000000. mean reward 76.917175.\n",
      "World Perf: Episode 919.000000. Reward 99.000000. action: 1.000000. mean reward 76.189690.\n",
      "World Perf: Episode 922.000000. Reward 114.333333. action: 1.000000. mean reward 75.456520.\n",
      "World Perf: Episode 925.000000. Reward 119.000000. action: 1.000000. mean reward 74.642395.\n",
      "World Perf: Episode 928.000000. Reward 115.000000. action: 0.000000. mean reward 99.987640.\n",
      "World Perf: Episode 931.000000. Reward 99.333333. action: 0.000000. mean reward 98.077301.\n",
      "World Perf: Episode 934.000000. Reward 130.000000. action: 0.000000. mean reward 102.530273.\n",
      "World Perf: Episode 937.000000. Reward 125.000000. action: 0.000000. mean reward 100.841331.\n",
      "World Perf: Episode 940.000000. Reward 137.000000. action: 1.000000. mean reward 101.483582.\n",
      "World Perf: Episode 943.000000. Reward 100.666667. action: 0.000000. mean reward 99.897560.\n",
      "World Perf: Episode 946.000000. Reward 110.333333. action: 1.000000. mean reward 100.638542.\n",
      "World Perf: Episode 949.000000. Reward 118.000000. action: 1.000000. mean reward 106.790169.\n",
      "World Perf: Episode 952.000000. Reward 190.000000. action: 0.000000. mean reward 105.918335.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7621f511b7e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Start displaying environment once performance is acceptably high.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m150\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdrawFromModel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrendering\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\traders_nlp\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\traders_nlp\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\traders_nlp\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\traders_nlp\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\traders_nlp\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\traders_nlp\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flip'"
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the model network and compute predicted_state                      #\n",
    "                # Output: 'pState'                                                             #\n",
    "                ################################################################################\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "\n",
    "            if trainThePolicy == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the policy network and compute newGrads                            #\n",
    "                # Output: 'tGrad'                                                              #\n",
    "                ################################################################################\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO:                                                                        #\n",
    "                    # (1) Run the policy network and update gradients                              #\n",
    "                    # (2) Reset gradBuffer to 0                                                    #\n",
    "                    ################################################################################\n",
    "                    switch_point = episode_number\n",
    "                    if trainThePolicy == True:\n",
    "                        sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                        gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01                  \n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes\n",
    "                if episode_number > 100:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Alternating between training the policy from the model and training    #\n",
    "                    # the model from the real environment.                                         #\n",
    "                    ################################################################################\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (traders_nlp)",
   "language": "python",
   "name": "traders_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
